<!DOCTYPE html><html lang="en" data-astro-cid-bvzihdzo> <head><!-- Global Metadata --><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1"><link rel="icon" type="image/svg+xml" href="/favicon.svg"><link rel="sitemap" href="/sitemap-index.xml"><link rel="alternate" type="application/rss+xml" title="NeuroScript Blog" href="https://severeon.github.io/rss.xml"><meta name="generator" content="Astro v5.16.3"><!-- Font preloads --><link rel="preload" href="/fonts/atkinson-regular.woff" as="font" type="font/woff" crossorigin><link rel="preload" href="/fonts/atkinson-bold.woff" as="font" type="font/woff" crossorigin><!-- Canonical URL --><link rel="canonical" href="https://severeon.github.io/blog/composable-pipelines/"><!-- Primary Meta Tags --><title>NeuroScript: Composable Pipelines for Neural Networks</title><meta name="title" content="NeuroScript: Composable Pipelines for Neural Networks"><meta name="description" content="Rethinking neural networks as middleware pipelines."><!-- Open Graph / Facebook --><meta property="og:type" content="website"><meta property="og:url" content="https://severeon.github.io/blog/composable-pipelines/"><meta property="og:title" content="NeuroScript: Composable Pipelines for Neural Networks"><meta property="og:description" content="Rethinking neural networks as middleware pipelines."><meta property="og:image" content="https://severeon.github.io/_astro/blog-placeholder-1.Bx0Zcyzv.jpg"><!-- Twitter --><meta property="twitter:card" content="summary_large_image"><meta property="twitter:url" content="https://severeon.github.io/blog/composable-pipelines/"><meta property="twitter:title" content="NeuroScript: Composable Pipelines for Neural Networks"><meta property="twitter:description" content="Rethinking neural networks as middleware pipelines."><meta property="twitter:image" content="https://severeon.github.io/_astro/blog-placeholder-1.Bx0Zcyzv.jpg"><style>:root{--accent: #2337ff;--accent-dark: #000d8a;--black: 15, 18, 25;--gray: 96, 115, 159;--gray-light: 229, 233, 240;--gray-dark: 34, 41, 57;--gray-gradient: rgba(var(--gray-light), 50%), #fff;--box-shadow: 0 2px 6px rgba(var(--gray), 25%), 0 8px 24px rgba(var(--gray), 33%), 0 16px 32px rgba(var(--gray), 33%)}@font-face{font-family:Atkinson;src:url(/fonts/atkinson-regular.woff) format("woff");font-weight:400;font-style:normal;font-display:swap}@font-face{font-family:Atkinson;src:url(/fonts/atkinson-bold.woff) format("woff");font-weight:700;font-style:normal;font-display:swap}body{font-family:Atkinson,sans-serif;margin:0;padding:0;text-align:left;background:linear-gradient(var(--gray-gradient)) no-repeat;background-size:100% 600px;word-wrap:break-word;overflow-wrap:break-word;color:rgb(var(--gray-dark));font-size:16px;line-height:1.6}main{width:720px;max-width:calc(100% - 2em);margin:auto;padding:3em 1em}h1,h2,h3,h4,h5,h6{margin:0 0 .5rem;color:rgb(var(--black));line-height:1.2}h1{font-size:3.052em}h2{font-size:2.441em}h3{font-size:1.953em}h4{font-size:1.563em}h5{font-size:1.25em}strong,b{font-weight:700}a,a:hover{color:var(--accent)}p{margin-bottom:1em}.prose p{margin-bottom:2em}textarea{width:100%;font-size:16px}input{font-size:16px}table{width:100%}img{max-width:100%;height:auto;border-radius:8px}code{padding:2px 5px;background-color:rgb(var(--gray-light));border-radius:2px;font-size:14px}pre{padding:1.5em;border-radius:8px}pre>code{all:unset}blockquote{border-left:4px solid var(--accent);padding:0 0 0 20px;margin:0;font-size:1.333em}hr{border:none;border-top:1px solid rgb(var(--gray-light))}@media(max-width:720px){body{font-size:18px}main{padding:1em}}.sr-only{border:0;padding:0;margin:0;position:absolute!important;height:1px;width:1px;overflow:hidden;clip:rect(1px 1px 1px 1px);clip:rect(1px,1px,1px,1px);clip-path:inset(50%);white-space:nowrap}footer[data-astro-cid-sz7xmlte]{padding:2em 1em 6em;background:linear-gradient(var(--gray-gradient)) no-repeat;color:rgb(var(--gray));text-align:center}.social-links[data-astro-cid-sz7xmlte]{display:flex;justify-content:center;gap:1em;margin-top:1em}.social-links[data-astro-cid-sz7xmlte] a[data-astro-cid-sz7xmlte]{text-decoration:none;color:rgb(var(--gray))}.social-links[data-astro-cid-sz7xmlte] a[data-astro-cid-sz7xmlte]:hover{color:rgb(var(--gray-dark))}a[data-astro-cid-eimmu3lg]{display:inline-block;text-decoration:none}a[data-astro-cid-eimmu3lg].active{font-weight:bolder;text-decoration:underline}header[data-astro-cid-3ef6ksr2]{margin:0;padding:0 1em;background:#fff;box-shadow:0 2px 8px rgba(var(--black),5%)}h2[data-astro-cid-3ef6ksr2]{margin:0;font-size:1em}h2[data-astro-cid-3ef6ksr2] a[data-astro-cid-3ef6ksr2],h2[data-astro-cid-3ef6ksr2] a[data-astro-cid-3ef6ksr2].active{text-decoration:none}nav[data-astro-cid-3ef6ksr2]{display:flex;align-items:center;justify-content:space-between}nav[data-astro-cid-3ef6ksr2] a[data-astro-cid-3ef6ksr2]{padding:1em .5em;color:var(--black);border-bottom:4px solid transparent;text-decoration:none}nav[data-astro-cid-3ef6ksr2] a[data-astro-cid-3ef6ksr2].active{text-decoration:none;border-bottom-color:var(--accent)}.social-links[data-astro-cid-3ef6ksr2],.social-links[data-astro-cid-3ef6ksr2] a[data-astro-cid-3ef6ksr2]{display:flex}@media(max-width:720px){.social-links[data-astro-cid-3ef6ksr2]{display:none}}
main[data-astro-cid-bvzihdzo]{width:calc(100% - 2em);max-width:100%;margin:0}.hero-image[data-astro-cid-bvzihdzo]{width:100%}.hero-image[data-astro-cid-bvzihdzo] img[data-astro-cid-bvzihdzo]{display:block;margin:0 auto;border-radius:12px;box-shadow:var(--box-shadow)}.prose[data-astro-cid-bvzihdzo]{width:720px;max-width:calc(100% - 2em);margin:auto;padding:1em;color:rgb(var(--gray-dark))}.title[data-astro-cid-bvzihdzo]{margin-bottom:1em;padding:1em 0;text-align:center;line-height:1}.title[data-astro-cid-bvzihdzo] h1[data-astro-cid-bvzihdzo]{margin:0 0 .5em}.date[data-astro-cid-bvzihdzo]{margin-bottom:.5em;color:rgb(var(--gray))}.last-updated-on[data-astro-cid-bvzihdzo]{font-style:italic}
</style></head> <body data-astro-cid-bvzihdzo> <header data-astro-cid-3ef6ksr2> <nav data-astro-cid-3ef6ksr2> <h2 data-astro-cid-3ef6ksr2><a href="/" data-astro-cid-3ef6ksr2>NeuroScript Blog</a></h2> <div class="internal-links" data-astro-cid-3ef6ksr2> <a href="/" data-astro-cid-3ef6ksr2="true" data-astro-cid-eimmu3lg> Home </a>  <a href="/about" data-astro-cid-3ef6ksr2="true" data-astro-cid-eimmu3lg> About </a>  </div> </nav> </header>  <main data-astro-cid-bvzihdzo> <article data-astro-cid-bvzihdzo> <div class="hero-image" data-astro-cid-bvzihdzo>  </div> <div class="prose" data-astro-cid-bvzihdzo> <div class="title" data-astro-cid-bvzihdzo> <div class="date" data-astro-cid-bvzihdzo> <time datetime="2024-11-01T05:00:00.000Z"> Nov 1, 2024 </time>  </div> <h1 data-astro-cid-bvzihdzo>NeuroScript: Composable Pipelines for Neural Networks</h1> <hr data-astro-cid-bvzihdzo> </div>  <p><strong>Why do I need to become an expert in mathematics and learn one of the least accessible Python libraries I’ve ever encountered just to experiment with transformers?</strong></p>
<p>I tried drawing a PlantUML flowchart to understand how state changes ripple through the system. Eight boxes in, it clicked: <strong>it’s just a pipeline with middleware</strong>.</p>
<p>But saying “10,000 disorganized notebooks and git repos totaling 200GB+ is blocking progress” feels inadequate. Magic is cool for a while, but programmers don’t like black boxes. At least this one doesn’t.</p>
<h3 id="the-core-idea">The Core Idea</h3>
<p><strong>Neurons</strong> are a powerful abstraction because they reduce granularity from <em>lines of PyTorch and math</em> to Lego-brick sized AI concepts. Everything is a neuron, neurons are made of neurons, and we’re running middleware on a pipeline moving arrays of floats through a computational landscape.</p>
<p><strong>Middleware</strong> takes input, applies a transformation, and passes the result to the next stage:</p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="text"><code><span class="line"><span>input -> [middleware, ...] -> output</span></span></code></pre>
<p>Simple, right?</p>
<p><strong>What if neural networks were actually <em>this</em> simple?</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="text"><code><span class="line"><span>in -> embedding -> FFN -> out</span></span></code></pre>
<h3 id="enter-neuroscript">Enter NeuroScript</h3>
<p>NeuroScript is a DSL that compiles to PyTorch. It treats neurons as first-class compositional primitives with explicit dataflow. Here’s GPT-2:</p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="bash"><code><span class="line"><span style="color:#B392F0">use</span><span style="color:#9ECBFF"> core,nn/</span><span style="color:#79B8FF">*</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D"># Complete GPT-style language model</span></span>
<span class="line"><span style="color:#B392F0">neuron</span><span style="color:#9ECBFF"> GPT</span><span style="color:#E1E4E8">(</span><span style="color:#B392F0">vocab_size,</span><span style="color:#9ECBFF"> d_model,</span><span style="color:#9ECBFF"> num_layers,</span><span style="color:#9ECBFF"> num_heads,</span><span style="color:#9ECBFF"> d_ff,</span><span style="color:#9ECBFF"> max_seq_len</span><span style="color:#E1E4E8">)</span><span style="color:#9ECBFF">:</span></span>
<span class="line"><span style="color:#E1E4E8">  in: [batch, seq]  </span><span style="color:#6A737D"># Token IDs</span></span>
<span class="line"><span style="color:#B392F0">  out:</span><span style="color:#E1E4E8"> [batch, </span><span style="color:#9ECBFF">seq,</span><span style="color:#9ECBFF"> vocab_size]</span><span style="color:#6A737D">  # Logits</span></span>
<span class="line"><span style="color:#79B8FF">  let</span><span style="color:#9ECBFF">:</span></span>
<span class="line"><span style="color:#B392F0">    layers</span><span style="color:#9ECBFF"> =</span><span style="color:#9ECBFF"> Sequential</span><span style="color:#E1E4E8">(</span><span style="color:#B392F0">num_layers,</span><span style="color:#9ECBFF"> TransformerBlock</span><span style="color:#E1E4E8">)</span></span>
<span class="line"><span style="color:#B392F0">  graph:</span></span>
<span class="line"><span style="color:#E1E4E8">    in -</span><span style="color:#F97583">></span></span>
<span class="line"><span style="color:#B392F0">      Embedding(vocab_size,</span><span style="color:#9ECBFF"> d_model</span><span style="color:#E1E4E8">)</span></span>
<span class="line"><span style="color:#B392F0">      embedded</span></span>
<span class="line"></span>
<span class="line"><span style="color:#B392F0">    embedded</span><span style="color:#E1E4E8"> -</span><span style="color:#F97583">></span></span>
<span class="line"><span style="color:#B392F0">      PositionalEncoding(d_model,</span><span style="color:#9ECBFF"> max_len=max_seq_len</span><span style="color:#E1E4E8">)</span></span>
<span class="line"><span style="color:#B392F0">      positioned</span></span>
<span class="line"></span>
<span class="line"><span style="color:#B392F0">    positioned</span><span style="color:#E1E4E8"> -</span><span style="color:#F97583">></span></span>
<span class="line"><span style="color:#B392F0">      layers(d_model,</span><span style="color:#9ECBFF"> num_heads,</span><span style="color:#9ECBFF"> d_ff</span><span style="color:#E1E4E8">)</span></span>
<span class="line"><span style="color:#B392F0">      features</span></span>
<span class="line"></span>
<span class="line"><span style="color:#B392F0">    features</span><span style="color:#E1E4E8"> -</span><span style="color:#F97583">></span></span>
<span class="line"><span style="color:#B392F0">      LayerNorm(d_model</span><span style="color:#E1E4E8">)</span></span>
<span class="line"><span style="color:#B392F0">      normalized</span></span>
<span class="line"></span>
<span class="line"><span style="color:#B392F0">    normalized</span><span style="color:#E1E4E8"> -</span><span style="color:#F97583">></span></span>
<span class="line"><span style="color:#B392F0">      Linear(d_model,</span><span style="color:#9ECBFF"> vocab_size</span><span style="color:#E1E4E8">)</span></span>
<span class="line"><span style="color:#B392F0">      out</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D"># Smaller test model for quick experiments</span></span>
<span class="line"><span style="color:#B392F0">neuron</span><span style="color:#9ECBFF"> TinyGPT</span><span style="color:#E1E4E8">(</span><span style="color:#B392F0">vocab_size</span><span style="color:#E1E4E8">)</span><span style="color:#9ECBFF">:</span></span>
<span class="line"><span style="color:#E1E4E8">  in: [batch, seq]</span></span>
<span class="line"><span style="color:#B392F0">  out:</span><span style="color:#E1E4E8"> [batch, </span><span style="color:#9ECBFF">seq,</span><span style="color:#9ECBFF"> vocab_size]</span></span>
<span class="line"><span style="color:#B392F0">  graph:</span></span>
<span class="line"><span style="color:#E1E4E8">    in -</span><span style="color:#F97583">></span><span style="color:#E1E4E8"> GPT(</span><span style="color:#B392F0">vocab_size,</span><span style="color:#9ECBFF"> 256,</span><span style="color:#9ECBFF"> 4,</span><span style="color:#9ECBFF"> 4,</span><span style="color:#9ECBFF"> 1024,</span><span style="color:#79B8FF"> 512</span><span style="color:#E1E4E8">) -</span><span style="color:#F97583">></span><span style="color:#E1E4E8"> out</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D"># GPT-2 Small configuration</span></span>
<span class="line"><span style="color:#B392F0">neuron</span><span style="color:#9ECBFF"> GPT2Small</span><span style="color:#E1E4E8">(</span><span style="color:#B392F0">vocab_size</span><span style="color:#E1E4E8">)</span><span style="color:#9ECBFF">:</span></span>
<span class="line"><span style="color:#E1E4E8">  in: [batch, seq]</span></span>
<span class="line"><span style="color:#B392F0">  out:</span><span style="color:#E1E4E8"> [batch, </span><span style="color:#9ECBFF">seq,</span><span style="color:#9ECBFF"> vocab_size]</span></span>
<span class="line"><span style="color:#B392F0">  graph:</span></span>
<span class="line"><span style="color:#E1E4E8">    in -</span><span style="color:#F97583">></span><span style="color:#E1E4E8"> GPT(</span><span style="color:#B392F0">vocab_size,</span><span style="color:#9ECBFF"> 768,</span><span style="color:#9ECBFF"> 12,</span><span style="color:#9ECBFF"> 12,</span><span style="color:#9ECBFF"> 3072,</span><span style="color:#79B8FF"> 1024</span><span style="color:#E1E4E8">) -</span><span style="color:#F97583">></span><span style="color:#E1E4E8"> out</span></span></code></pre>
<p>That’s it. Compare this to <a href="https://github.com/karpathy/nanoGPT/blob/master/model.py">any PyTorch GPT-2 implementation</a>.</p>
<h3 id="whats-next">What’s Next</h3>
<p>I’m building this to make architecture exploration feel like playing with Legos instead of reading textbooks. Still figuring out details around recursive architectures and shape inference.</p>
<p>Like what you see? Maybe you see obvious problems I’m missing… I genuinely want to know!</p>
<hr>
<p><em>NeuroScript is a research project in active development. Implementation details are evolving.</em></p>  </div> </article> </main> <footer data-astro-cid-sz7xmlte>
&copy; 2025 Thomas Quick. All rights reserved.
</footer>  </body></html>
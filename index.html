<!DOCTYPE html><html lang="en" data-astro-cid-j7pv25f6> <head><!-- Global Metadata --><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1"><link rel="icon" type="image/svg+xml" href="/favicon.svg"><link rel="sitemap" href="/sitemap-index.xml"><link rel="alternate" type="application/rss+xml" title="NeuroScript Blog" href="https://severeon.github.io/rss.xml"><meta name="generator" content="Astro v5.16.3"><!-- Font preloads --><link rel="preload" href="/fonts/atkinson-regular.woff" as="font" type="font/woff" crossorigin><link rel="preload" href="/fonts/atkinson-bold.woff" as="font" type="font/woff" crossorigin><!-- Canonical URL --><link rel="canonical" href="https://severeon.github.io/"><!-- Primary Meta Tags --><title>NeuroScript Blog</title><meta name="title" content="NeuroScript Blog"><meta name="description" content="Thoughts on AI, NeuroScript, and programming."><!-- Open Graph / Facebook --><meta property="og:type" content="website"><meta property="og:url" content="https://severeon.github.io/"><meta property="og:title" content="NeuroScript Blog"><meta property="og:description" content="Thoughts on AI, NeuroScript, and programming."><meta property="og:image" content="https://severeon.github.io/_astro/blog-placeholder-1.Bx0Zcyzv.jpg"><!-- Twitter --><meta property="twitter:card" content="summary_large_image"><meta property="twitter:url" content="https://severeon.github.io/"><meta property="twitter:title" content="NeuroScript Blog"><meta property="twitter:description" content="Thoughts on AI, NeuroScript, and programming."><meta property="twitter:image" content="https://severeon.github.io/_astro/blog-placeholder-1.Bx0Zcyzv.jpg"><style>:root{--accent: #2337ff;--accent-dark: #000d8a;--black: 15, 18, 25;--gray: 96, 115, 159;--gray-light: 229, 233, 240;--gray-dark: 34, 41, 57;--gray-gradient: rgba(var(--gray-light), 50%), #fff;--box-shadow: 0 2px 6px rgba(var(--gray), 25%), 0 8px 24px rgba(var(--gray), 33%), 0 16px 32px rgba(var(--gray), 33%)}@font-face{font-family:Atkinson;src:url(/fonts/atkinson-regular.woff) format("woff");font-weight:400;font-style:normal;font-display:swap}@font-face{font-family:Atkinson;src:url(/fonts/atkinson-bold.woff) format("woff");font-weight:700;font-style:normal;font-display:swap}body{font-family:Atkinson,sans-serif;margin:0;padding:0;text-align:left;background:linear-gradient(var(--gray-gradient)) no-repeat;background-size:100% 600px;word-wrap:break-word;overflow-wrap:break-word;color:rgb(var(--gray-dark));font-size:16px;line-height:1.6}main{width:720px;max-width:calc(100% - 2em);margin:auto;padding:3em 1em}h1,h2,h3,h4,h5,h6{margin:0 0 .5rem;color:rgb(var(--black));line-height:1.2}h1{font-size:3.052em}h2{font-size:2.441em}h3{font-size:1.953em}h4{font-size:1.563em}h5{font-size:1.25em}strong,b{font-weight:700}a,a:hover{color:var(--accent)}p{margin-bottom:1em}.prose p{margin-bottom:2em}textarea{width:100%;font-size:16px}input{font-size:16px}table{width:100%}img{max-width:100%;height:auto;border-radius:8px}code{padding:2px 5px;background-color:rgb(var(--gray-light));border-radius:2px;font-size:14px}pre{padding:1.5em;border-radius:8px}pre>code{all:unset}blockquote{border-left:4px solid var(--accent);padding:0 0 0 20px;margin:0;font-size:1.333em}hr{border:none;border-top:1px solid rgb(var(--gray-light))}@media(max-width:720px){body{font-size:18px}main{padding:1em}}.sr-only{border:0;padding:0;margin:0;position:absolute!important;height:1px;width:1px;overflow:hidden;clip:rect(1px 1px 1px 1px);clip:rect(1px,1px,1px,1px);clip-path:inset(50%);white-space:nowrap}footer[data-astro-cid-sz7xmlte]{padding:2em 1em 6em;background:linear-gradient(var(--gray-gradient)) no-repeat;color:rgb(var(--gray));text-align:center}.social-links[data-astro-cid-sz7xmlte]{display:flex;justify-content:center;gap:1em;margin-top:1em}.social-links[data-astro-cid-sz7xmlte] a[data-astro-cid-sz7xmlte]{text-decoration:none;color:rgb(var(--gray))}.social-links[data-astro-cid-sz7xmlte] a[data-astro-cid-sz7xmlte]:hover{color:rgb(var(--gray-dark))}a[data-astro-cid-eimmu3lg]{display:inline-block;text-decoration:none}a[data-astro-cid-eimmu3lg].active{font-weight:bolder;text-decoration:underline}header[data-astro-cid-3ef6ksr2]{margin:0;padding:0 1em;background:#fff;box-shadow:0 2px 8px rgba(var(--black),5%)}h2[data-astro-cid-3ef6ksr2]{margin:0;font-size:1em}h2[data-astro-cid-3ef6ksr2] a[data-astro-cid-3ef6ksr2],h2[data-astro-cid-3ef6ksr2] a[data-astro-cid-3ef6ksr2].active{text-decoration:none}nav[data-astro-cid-3ef6ksr2]{display:flex;align-items:center;justify-content:space-between}nav[data-astro-cid-3ef6ksr2] a[data-astro-cid-3ef6ksr2]{padding:1em .5em;color:var(--black);border-bottom:4px solid transparent;text-decoration:none}nav[data-astro-cid-3ef6ksr2] a[data-astro-cid-3ef6ksr2].active{text-decoration:none;border-bottom-color:var(--accent)}.social-links[data-astro-cid-3ef6ksr2],.social-links[data-astro-cid-3ef6ksr2] a[data-astro-cid-3ef6ksr2]{display:flex}@media(max-width:720px){.social-links[data-astro-cid-3ef6ksr2]{display:none}}
main[data-astro-cid-j7pv25f6]{width:960px}.intro[data-astro-cid-j7pv25f6]{margin-bottom:3rem;text-align:center}.post[data-astro-cid-j7pv25f6]{margin-bottom:4rem;padding-bottom:2rem;border-bottom:1px solid rgb(var(--gray-light))}.post[data-astro-cid-j7pv25f6]:last-child{border-bottom:none}.title[data-astro-cid-j7pv25f6]{margin:0;color:rgb(var(--black));line-height:1}.date[data-astro-cid-j7pv25f6]{margin:0;color:rgb(var(--gray));margin-bottom:1rem}.content[data-astro-cid-j7pv25f6]{margin-top:1rem}
</style></head> <body data-astro-cid-j7pv25f6> <header data-astro-cid-3ef6ksr2> <nav data-astro-cid-3ef6ksr2> <h2 data-astro-cid-3ef6ksr2><a href="/" data-astro-cid-3ef6ksr2>NeuroScript Blog</a></h2> <div class="internal-links" data-astro-cid-3ef6ksr2> <a href="/" class="active" data-astro-cid-3ef6ksr2="true" data-astro-cid-eimmu3lg> Home </a>  <a href="/about" data-astro-cid-3ef6ksr2="true" data-astro-cid-eimmu3lg> About </a>  </div> </nav> </header>  <main data-astro-cid-j7pv25f6> <div class="intro" data-astro-cid-j7pv25f6> <h1 data-astro-cid-j7pv25f6>NeuroScript Blog</h1> <p data-astro-cid-j7pv25f6>
Thoughts on AI, NeuroScript, and programming.
</p> </div> <section data-astro-cid-j7pv25f6> <article class="post" data-astro-cid-j7pv25f6> <a href="/blog/tuple-unpacking/" style="text-decoration: none;" data-astro-cid-j7pv25f6> <h2 class="title" data-astro-cid-j7pv25f6>Tuple Unpacking: Skips that Click</h2> </a> <p class="date" data-astro-cid-j7pv25f6> <time datetime="2024-12-02T06:00:00.000Z"> Dec 2, 2024 </time> </p>  <div class="content" data-astro-cid-j7pv25f6> <p><strong>NeuroScript makes skip connections obvious with tuple unpacking:</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="bash"><code><span class="line"><span style="color:#B392F0">neuron</span><span style="color:#9ECBFF"> Residual</span><span style="color:#E1E4E8">(</span><span style="color:#B392F0">dim</span><span style="color:#E1E4E8">)</span><span style="color:#9ECBFF">:</span></span>
<span class="line"><span style="color:#E1E4E8">  in: [</span><span style="color:#F97583">*</span><span style="color:#E1E4E8">, dim]</span></span>
<span class="line"><span style="color:#B392F0">  out:</span><span style="color:#E1E4E8"> [*, </span><span style="color:#9ECBFF">dim]</span></span>
<span class="line"><span style="color:#B392F0">  graph:</span></span>
<span class="line"><span style="color:#6A737D">    # Fork the input into two paths</span></span>
<span class="line"><span style="color:#E1E4E8">    in -</span><span style="color:#F97583">></span><span style="color:#B392F0"> Fork</span><span style="color:#E1E4E8">() -</span><span style="color:#F97583">></span><span style="color:#E1E4E8"> (</span><span style="color:#B392F0">main,</span><span style="color:#9ECBFF"> skip</span><span style="color:#E1E4E8">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D">    # Transform one path</span></span>
<span class="line"><span style="color:#B392F0">    main</span><span style="color:#E1E4E8"> -</span><span style="color:#F97583">></span><span style="color:#9ECBFF"> MLP</span><span style="color:#E1E4E8">(</span><span style="color:#B392F0">dim</span><span style="color:#E1E4E8">) -</span><span style="color:#F97583">></span><span style="color:#9ECBFF"> processed</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D">    # Add them back together</span></span>
<span class="line"><span style="color:#E1E4E8">    (</span><span style="color:#B392F0">processed,</span><span style="color:#9ECBFF"> skip</span><span style="color:#E1E4E8">) </span><span style="color:#B392F0">-</span><span style="color:#E1E4E8">> </span><span style="color:#9ECBFF">Add</span><span style="color:#E1E4E8">() -</span><span style="color:#F97583">></span><span style="color:#9ECBFF"> out</span></span></code></pre>
<p><strong>What’s happening?</strong></p>
<ol>
<li><code>Fork()</code> takes one tensor, outputs a tuple of two identical tensors</li>
<li><code>(main, skip)</code> <strong>unpacks</strong> that tuple into named references</li>
<li>You can now route <code>main</code> through layers while <code>skip</code> waits</li>
<li><code>(processed, skip)</code> <strong>packs</strong> them back into a tuple for <code>Add()</code></li>
</ol>
<p>The dataflow is <strong>explicit</strong>. No mental juggling of variable lifetimes. You can <em>see</em> the skip connection in the graph structure.</p>
<p><strong>This extends beautifully to complex patterns:</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="bash"><code><span class="line"><span style="color:#B392F0">neuron</span><span style="color:#9ECBFF"> TransformerBlock</span><span style="color:#E1E4E8">(</span><span style="color:#B392F0">d_model,</span><span style="color:#9ECBFF"> num_heads,</span><span style="color:#9ECBFF"> d_ff</span><span style="color:#E1E4E8">)</span><span style="color:#9ECBFF">:</span></span>
<span class="line"><span style="color:#E1E4E8">  in: [batch, seq, d_model]</span></span>
<span class="line"><span style="color:#B392F0">  out:</span><span style="color:#E1E4E8"> [batch, </span><span style="color:#9ECBFF">seq,</span><span style="color:#9ECBFF"> d_model]</span></span>
<span class="line"><span style="color:#B392F0">  graph:</span></span>
<span class="line"><span style="color:#6A737D">    # Attention with skip</span></span>
<span class="line"><span style="color:#E1E4E8">    in -</span><span style="color:#F97583">></span><span style="color:#B392F0"> Fork</span><span style="color:#E1E4E8">() -</span><span style="color:#F97583">></span><span style="color:#E1E4E8"> (</span><span style="color:#B392F0">attn_input,</span><span style="color:#9ECBFF"> skip1</span><span style="color:#E1E4E8">)</span></span>
<span class="line"><span style="color:#B392F0">    attn_input</span><span style="color:#E1E4E8"> -</span><span style="color:#F97583">></span></span>
<span class="line"><span style="color:#B392F0">      MultiHeadAttention(d_model,</span><span style="color:#9ECBFF"> num_heads</span><span style="color:#E1E4E8">)</span></span>
<span class="line"><span style="color:#B392F0">      attn_out</span></span>
<span class="line"><span style="color:#E1E4E8">    (</span><span style="color:#B392F0">attn_out,</span><span style="color:#9ECBFF"> skip1</span><span style="color:#E1E4E8">) </span><span style="color:#B392F0">-</span><span style="color:#E1E4E8">> </span><span style="color:#9ECBFF">Add</span><span style="color:#E1E4E8">() -</span><span style="color:#F97583">></span><span style="color:#9ECBFF"> attn_residual</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D">    # Layer norm</span></span>
<span class="line"><span style="color:#B392F0">    attn_residual</span><span style="color:#E1E4E8"> -</span><span style="color:#F97583">></span><span style="color:#9ECBFF"> LayerNorm</span><span style="color:#E1E4E8">(</span><span style="color:#B392F0">d_model</span><span style="color:#E1E4E8">) -</span><span style="color:#F97583">></span><span style="color:#9ECBFF"> normalized1</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D">    # FFN with skip</span></span>
<span class="line"><span style="color:#B392F0">    normalized1</span><span style="color:#E1E4E8"> -</span><span style="color:#F97583">></span><span style="color:#9ECBFF"> Fork</span><span style="color:#E1E4E8">() -</span><span style="color:#F97583">></span><span style="color:#E1E4E8"> (ffn_input, </span><span style="color:#9ECBFF">skip2</span><span style="color:#E1E4E8">)</span></span>
<span class="line"><span style="color:#B392F0">    ffn_input</span><span style="color:#E1E4E8"> -</span><span style="color:#F97583">></span><span style="color:#9ECBFF"> FFN</span><span style="color:#E1E4E8">(</span><span style="color:#B392F0">d_model,</span><span style="color:#9ECBFF"> d_ff</span><span style="color:#E1E4E8">) -</span><span style="color:#F97583">></span><span style="color:#9ECBFF"> ffn_out</span></span>
<span class="line"><span style="color:#E1E4E8">    (</span><span style="color:#B392F0">ffn_out,</span><span style="color:#9ECBFF"> skip2</span><span style="color:#E1E4E8">) </span><span style="color:#B392F0">-</span><span style="color:#E1E4E8">> </span><span style="color:#9ECBFF">Add</span><span style="color:#E1E4E8">() -</span><span style="color:#F97583">></span><span style="color:#9ECBFF"> ffn_residual</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D">    # Final layer norm</span></span>
<span class="line"><span style="color:#B392F0">    ffn_residual</span><span style="color:#E1E4E8"> -</span><span style="color:#F97583">></span><span style="color:#9ECBFF"> LayerNorm</span><span style="color:#E1E4E8">(</span><span style="color:#B392F0">d_model</span><span style="color:#E1E4E8">) -</span><span style="color:#F97583">></span><span style="color:#9ECBFF"> out</span></span></code></pre>
<p><strong>The pattern is crystal clear:</strong></p>
<ol>
<li>Fork -> (process, skip)</li>
<li>Do work on <code>process</code></li>
<li>(result, skip) -> Add</li>
<li>Repeat</li>
</ol>
<p>Compare this to tracking <code>x_0</code>, <code>x_1</code>, <code>residual_1</code>, <code>residual_2</code> in PyTorch. NeuroScript’s tuple unpacking makes the <strong>structure</strong> of the network match the <strong>intent</strong> of the architecture.</p>
<p>When you see <code>(a, b, c)</code> in the graph, you know exactly what’s happening: dataflow is splitting or merging. No hidden state, no variable confusion.</p> </div> </article><article class="post" data-astro-cid-j7pv25f6> <a href="/blog/training-made-simple/" style="text-decoration: none;" data-astro-cid-j7pv25f6> <h2 class="title" data-astro-cid-j7pv25f6>Convention Over Configuration</h2> </a> <p class="date" data-astro-cid-j7pv25f6> <time datetime="2024-12-01T06:00:00.000Z"> Dec 1, 2024 </time> </p>  <div class="content" data-astro-cid-j7pv25f6> <p><strong>Here’s what frustrated me about ML frameworks:</strong></p>
<ul>
<li>PyTorch: Too low-level, write your own training loops</li>
<li>PyTorch Lightning: Better, but boilerplate-heavy</li>
<li>Keras: Great API, but locked to TensorFlow (historically)</li>
<li>Hugging Face: Amazing for transformers, but domain-specific</li>
</ul>
<p><strong>I wanted:</strong> Write the architecture, point to data, get training. No boilerplate.</p>
<p><strong>NeuroScript now has this.</strong> Here’s training XOR:</p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="bash"><code><span class="line"><span style="color:#6A737D"># 1. Write the architecture (01-xor.ns)</span></span>
<span class="line"><span style="color:#B392F0">neuron</span><span style="color:#9ECBFF"> XOR</span><span style="color:#E1E4E8">()</span><span style="color:#9ECBFF">:</span></span>
<span class="line"><span style="color:#E1E4E8">  in: [batch, 2]</span></span>
<span class="line"><span style="color:#B392F0">  out:</span><span style="color:#E1E4E8"> [batch, </span><span style="color:#9ECBFF">1]</span></span>
<span class="line"><span style="color:#B392F0">  graph:</span></span>
<span class="line"><span style="color:#E1E4E8">    in -</span><span style="color:#F97583">></span></span>
<span class="line"><span style="color:#B392F0">      Linear(2,</span><span style="color:#79B8FF"> 4</span><span style="color:#E1E4E8">)</span></span>
<span class="line"><span style="color:#B392F0">      ReLU</span><span style="color:#E1E4E8">()</span></span>
<span class="line"><span style="color:#B392F0">      Linear(4,</span><span style="color:#79B8FF"> 1</span><span style="color:#E1E4E8">)</span></span>
<span class="line"><span style="color:#B392F0">      Sigmoid</span><span style="color:#E1E4E8">()</span></span>
<span class="line"><span style="color:#B392F0">      out</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D"># 2. Create training data (xor_train.jsonl)</span></span>
<span class="line"><span style="color:#E1E4E8">{</span><span style="color:#B392F0">"input"</span><span style="color:#79B8FF">:</span><span style="color:#E1E4E8"> [0.0, </span><span style="color:#9ECBFF">0.0],</span><span style="color:#9ECBFF"> "target":</span><span style="color:#E1E4E8"> [0.0]}</span></span>
<span class="line"><span style="color:#E1E4E8">{</span><span style="color:#B392F0">"input"</span><span style="color:#79B8FF">:</span><span style="color:#E1E4E8"> [0.0, </span><span style="color:#9ECBFF">1.0],</span><span style="color:#9ECBFF"> "target":</span><span style="color:#E1E4E8"> [1.0]}</span></span>
<span class="line"><span style="color:#E1E4E8">{</span><span style="color:#B392F0">"input"</span><span style="color:#79B8FF">:</span><span style="color:#E1E4E8"> [1.0, </span><span style="color:#9ECBFF">0.0],</span><span style="color:#9ECBFF"> "target":</span><span style="color:#E1E4E8"> [1.0]}</span></span>
<span class="line"><span style="color:#E1E4E8">{</span><span style="color:#B392F0">"input"</span><span style="color:#79B8FF">:</span><span style="color:#E1E4E8"> [1.0, </span><span style="color:#9ECBFF">1.0],</span><span style="color:#9ECBFF"> "target":</span><span style="color:#E1E4E8"> [0.0]}</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D"># 3. Write a minimal config (xor_config.yml)</span></span>
<span class="line"><span style="color:#B392F0">model:</span></span>
<span class="line"><span style="color:#B392F0">  neuron:</span><span style="color:#9ECBFF"> XOR</span></span>
<span class="line"><span style="color:#B392F0">  file:</span><span style="color:#9ECBFF"> examples/01-xor.ns</span></span>
<span class="line"></span>
<span class="line"><span style="color:#B392F0">data:</span></span>
<span class="line"><span style="color:#B392F0">  train:</span><span style="color:#9ECBFF"> examples/data/xor_train.jsonl</span></span>
<span class="line"></span>
<span class="line"><span style="color:#B392F0">training:</span></span>
<span class="line"><span style="color:#B392F0">  epochs:</span><span style="color:#79B8FF"> 1000</span></span>
<span class="line"><span style="color:#B392F0">  lr:</span><span style="color:#79B8FF"> 0.01</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D"># 4. Train</span></span>
<span class="line"><span style="color:#B392F0">python</span><span style="color:#79B8FF"> -m</span><span style="color:#9ECBFF"> neuroscript_runtime.runner</span><span style="color:#9ECBFF"> train</span><span style="color:#79B8FF"> --config</span><span style="color:#9ECBFF"> xor_config.yml</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D"># That's it!</span></span></code></pre>
<p><strong>What makes this work? <em>Convention over Configuration</em></strong></p>
<p>The runner:</p>
<ol>
<li>
<p><strong>Infers the task</strong> from input/output shapes:</p>
<ul>
<li><code>[batch, 2] -> [batch, 1]</code> = Regression -> MSE loss</li>
<li><code>[batch, seq] -> [batch, seq, vocab]</code> = Language Model -> CrossEntropy</li>
<li><code>[batch, C, H, W] -> [batch, classes]</code> = Image Classification</li>
</ul>
</li>
<li>
<p><strong>Picks sensible defaults:</strong></p>
<ul>
<li>Optimizer: Adam (good for most things)</li>
<li>Batch size: 32</li>
<li>Logging: Every 100 steps</li>
<li>Checkpointing: Every 1000 steps</li>
</ul>
</li>
<li>
<p><strong>Makes extension trivial:</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#F97583">from</span><span style="color:#E1E4E8"> neuroscript_runtime.contracts </span><span style="color:#F97583">import</span><span style="color:#E1E4E8"> DataLoaderContract, ContractRegistry</span></span>
<span class="line"></span>
<span class="line"><span style="color:#F97583">class</span><span style="color:#B392F0"> MyHuggingFaceLoader</span><span style="color:#E1E4E8">(</span><span style="color:#B392F0">DataLoaderContract</span><span style="color:#E1E4E8">):</span></span>
<span class="line"><span style="color:#6A737D">    # Implement interface</span></span>
<span class="line"><span style="color:#F97583">    pass</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D"># Register it</span></span>
<span class="line"><span style="color:#E1E4E8">ContractRegistry.register_dataloader(</span><span style="color:#9ECBFF">"huggingface"</span><span style="color:#E1E4E8">, MyHuggingFaceLoader)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D"># Use in config:</span></span>
<span class="line"><span style="color:#6A737D"># data:</span></span>
<span class="line"><span style="color:#6A737D">#   format: huggingface</span></span>
<span class="line"><span style="color:#6A737D">#   dataset: "wikitext"</span></span></code></pre>
</li>
</ol>
<p><strong>The Contract System</strong> is the secret sauce. Five extension points:</p>
<ul>
<li><strong>DataLoader</strong>: How to load data (default: JSONL files)</li>
<li><strong>Loss</strong>: How to compute error (default: inferred from task)</li>
<li><strong>Optimizer</strong>: How to update weights (default: Adam)</li>
<li><strong>Checkpoint</strong>: How to save/load (default: <code>torch.save</code>)</li>
<li><strong>Logger</strong>: How to track progress (default: console)</li>
</ul>
<p>Ship with one good default for each. Make it trivial to swap in custom implementations. Let the community build the ecosystem.</p>
<p><strong>The full Python API:</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#F97583">from</span><span style="color:#E1E4E8"> neuroscript_runtime.runner_v2 </span><span style="color:#F97583">import</span><span style="color:#E1E4E8"> train_from_config</span></span>
<span class="line"><span style="color:#F97583">from</span><span style="color:#E1E4E8"> xor_model </span><span style="color:#F97583">import</span><span style="color:#79B8FF"> XOR</span><span style="color:#6A737D">  # Generated by NeuroScript compiler</span></span>
<span class="line"></span>
<span class="line"><span style="color:#E1E4E8">model </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> XOR()</span></span>
<span class="line"><span style="color:#E1E4E8">runner </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> train_from_config(model, </span><span style="color:#9ECBFF">"config.yml"</span><span style="color:#E1E4E8">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D"># Inference</span></span>
<span class="line"><span style="color:#F97583">import</span><span style="color:#E1E4E8"> torch</span></span>
<span class="line"><span style="color:#E1E4E8">result </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> runner.infer(torch.tensor([[</span><span style="color:#79B8FF">1.0</span><span style="color:#E1E4E8">, </span><span style="color:#79B8FF">0.0</span><span style="color:#E1E4E8">]]))</span></span>
<span class="line"><span style="color:#79B8FF">print</span><span style="color:#E1E4E8">(result)  </span><span style="color:#6A737D"># [0.9999] ≈ 1.0</span></span></code></pre>
<p><strong>Why this matters:</strong> You can go from idea to trained model in minutes, not hours. When you need custom behavior, the extension points are obvious. When you need full control, it’s just PyTorch under the hood—drop down anytime.</p>
<p><strong>Batteries included. Escape hatch provided.</strong></p> </div> </article><article class="post" data-astro-cid-j7pv25f6> <a href="/blog/composable-pipelines/" style="text-decoration: none;" data-astro-cid-j7pv25f6> <h2 class="title" data-astro-cid-j7pv25f6>NeuroScript: Composable Pipelines for Neural Networks</h2> </a> <p class="date" data-astro-cid-j7pv25f6> <time datetime="2024-11-01T05:00:00.000Z"> Nov 1, 2024 </time> </p>  <div class="content" data-astro-cid-j7pv25f6> <p><strong>Why do I need to become an expert in mathematics and learn one of the least accessible Python libraries I’ve ever encountered just to experiment with transformers?</strong></p>
<p>I tried drawing a PlantUML flowchart to understand how state changes ripple through the system. Eight boxes in, it clicked: <strong>it’s just a pipeline with middleware</strong>.</p>
<p>But saying “10,000 disorganized notebooks and git repos totaling 200GB+ is blocking progress” feels inadequate. Magic is cool for a while, but programmers don’t like black boxes. At least this one doesn’t.</p>
<h3 id="the-core-idea">The Core Idea</h3>
<p><strong>Neurons</strong> are a powerful abstraction because they reduce granularity from <em>lines of PyTorch and math</em> to Lego-brick sized AI concepts. Everything is a neuron, neurons are made of neurons, and we’re running middleware on a pipeline moving arrays of floats through a computational landscape.</p>
<p><strong>Middleware</strong> takes input, applies a transformation, and passes the result to the next stage:</p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="text"><code><span class="line"><span>input -> [middleware, ...] -> output</span></span></code></pre>
<p>Simple, right?</p>
<p><strong>What if neural networks were actually <em>this</em> simple?</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="text"><code><span class="line"><span>in -> embedding -> FFN -> out</span></span></code></pre>
<h3 id="enter-neuroscript">Enter NeuroScript</h3>
<p>NeuroScript is a DSL that compiles to PyTorch. It treats neurons as first-class compositional primitives with explicit dataflow. Here’s GPT-2:</p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="bash"><code><span class="line"><span style="color:#B392F0">use</span><span style="color:#9ECBFF"> core,nn/</span><span style="color:#79B8FF">*</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D"># Complete GPT-style language model</span></span>
<span class="line"><span style="color:#B392F0">neuron</span><span style="color:#9ECBFF"> GPT</span><span style="color:#E1E4E8">(</span><span style="color:#B392F0">vocab_size,</span><span style="color:#9ECBFF"> d_model,</span><span style="color:#9ECBFF"> num_layers,</span><span style="color:#9ECBFF"> num_heads,</span><span style="color:#9ECBFF"> d_ff,</span><span style="color:#9ECBFF"> max_seq_len</span><span style="color:#E1E4E8">)</span><span style="color:#9ECBFF">:</span></span>
<span class="line"><span style="color:#E1E4E8">  in: [batch, seq]  </span><span style="color:#6A737D"># Token IDs</span></span>
<span class="line"><span style="color:#B392F0">  out:</span><span style="color:#E1E4E8"> [batch, </span><span style="color:#9ECBFF">seq,</span><span style="color:#9ECBFF"> vocab_size]</span><span style="color:#6A737D">  # Logits</span></span>
<span class="line"><span style="color:#79B8FF">  let</span><span style="color:#9ECBFF">:</span></span>
<span class="line"><span style="color:#B392F0">    layers</span><span style="color:#9ECBFF"> =</span><span style="color:#9ECBFF"> Sequential</span><span style="color:#E1E4E8">(</span><span style="color:#B392F0">num_layers,</span><span style="color:#9ECBFF"> TransformerBlock</span><span style="color:#E1E4E8">)</span></span>
<span class="line"><span style="color:#B392F0">  graph:</span></span>
<span class="line"><span style="color:#E1E4E8">    in -</span><span style="color:#F97583">></span></span>
<span class="line"><span style="color:#B392F0">      Embedding(vocab_size,</span><span style="color:#9ECBFF"> d_model</span><span style="color:#E1E4E8">)</span></span>
<span class="line"><span style="color:#B392F0">      embedded</span></span>
<span class="line"></span>
<span class="line"><span style="color:#B392F0">    embedded</span><span style="color:#E1E4E8"> -</span><span style="color:#F97583">></span></span>
<span class="line"><span style="color:#B392F0">      PositionalEncoding(d_model,</span><span style="color:#9ECBFF"> max_len=max_seq_len</span><span style="color:#E1E4E8">)</span></span>
<span class="line"><span style="color:#B392F0">      positioned</span></span>
<span class="line"></span>
<span class="line"><span style="color:#B392F0">    positioned</span><span style="color:#E1E4E8"> -</span><span style="color:#F97583">></span></span>
<span class="line"><span style="color:#B392F0">      layers(d_model,</span><span style="color:#9ECBFF"> num_heads,</span><span style="color:#9ECBFF"> d_ff</span><span style="color:#E1E4E8">)</span></span>
<span class="line"><span style="color:#B392F0">      features</span></span>
<span class="line"></span>
<span class="line"><span style="color:#B392F0">    features</span><span style="color:#E1E4E8"> -</span><span style="color:#F97583">></span></span>
<span class="line"><span style="color:#B392F0">      LayerNorm(d_model</span><span style="color:#E1E4E8">)</span></span>
<span class="line"><span style="color:#B392F0">      normalized</span></span>
<span class="line"></span>
<span class="line"><span style="color:#B392F0">    normalized</span><span style="color:#E1E4E8"> -</span><span style="color:#F97583">></span></span>
<span class="line"><span style="color:#B392F0">      Linear(d_model,</span><span style="color:#9ECBFF"> vocab_size</span><span style="color:#E1E4E8">)</span></span>
<span class="line"><span style="color:#B392F0">      out</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D"># Smaller test model for quick experiments</span></span>
<span class="line"><span style="color:#B392F0">neuron</span><span style="color:#9ECBFF"> TinyGPT</span><span style="color:#E1E4E8">(</span><span style="color:#B392F0">vocab_size</span><span style="color:#E1E4E8">)</span><span style="color:#9ECBFF">:</span></span>
<span class="line"><span style="color:#E1E4E8">  in: [batch, seq]</span></span>
<span class="line"><span style="color:#B392F0">  out:</span><span style="color:#E1E4E8"> [batch, </span><span style="color:#9ECBFF">seq,</span><span style="color:#9ECBFF"> vocab_size]</span></span>
<span class="line"><span style="color:#B392F0">  graph:</span></span>
<span class="line"><span style="color:#E1E4E8">    in -</span><span style="color:#F97583">></span><span style="color:#E1E4E8"> GPT(</span><span style="color:#B392F0">vocab_size,</span><span style="color:#9ECBFF"> 256,</span><span style="color:#9ECBFF"> 4,</span><span style="color:#9ECBFF"> 4,</span><span style="color:#9ECBFF"> 1024,</span><span style="color:#79B8FF"> 512</span><span style="color:#E1E4E8">) -</span><span style="color:#F97583">></span><span style="color:#E1E4E8"> out</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D"># GPT-2 Small configuration</span></span>
<span class="line"><span style="color:#B392F0">neuron</span><span style="color:#9ECBFF"> GPT2Small</span><span style="color:#E1E4E8">(</span><span style="color:#B392F0">vocab_size</span><span style="color:#E1E4E8">)</span><span style="color:#9ECBFF">:</span></span>
<span class="line"><span style="color:#E1E4E8">  in: [batch, seq]</span></span>
<span class="line"><span style="color:#B392F0">  out:</span><span style="color:#E1E4E8"> [batch, </span><span style="color:#9ECBFF">seq,</span><span style="color:#9ECBFF"> vocab_size]</span></span>
<span class="line"><span style="color:#B392F0">  graph:</span></span>
<span class="line"><span style="color:#E1E4E8">    in -</span><span style="color:#F97583">></span><span style="color:#E1E4E8"> GPT(</span><span style="color:#B392F0">vocab_size,</span><span style="color:#9ECBFF"> 768,</span><span style="color:#9ECBFF"> 12,</span><span style="color:#9ECBFF"> 12,</span><span style="color:#9ECBFF"> 3072,</span><span style="color:#79B8FF"> 1024</span><span style="color:#E1E4E8">) -</span><span style="color:#F97583">></span><span style="color:#E1E4E8"> out</span></span></code></pre>
<p>That’s it. Compare this to <a href="https://github.com/karpathy/nanoGPT/blob/master/model.py">any PyTorch GPT-2 implementation</a>.</p>
<h3 id="whats-next">What’s Next</h3>
<p>I’m building this to make architecture exploration feel like playing with Legos instead of reading textbooks. Still figuring out details around recursive architectures and shape inference.</p>
<p>Like what you see? Maybe you see obvious problems I’m missing… I genuinely want to know!</p>
<hr>
<p><em>NeuroScript is a research project in active development. Implementation details are evolving.</em></p> </div> </article> </section> </main> <footer data-astro-cid-sz7xmlte>
&copy; 2025 Thomas Quick. All rights reserved.
</footer>  </body></html>
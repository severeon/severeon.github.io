"use strict";(globalThis.webpackChunkneuroscript_docs=globalThis.webpackChunkneuroscript_docs||[]).push([[2355],{551(e,n,i){i.r(n),i.d(n,{assets:()=>c,contentTitle:()=>o,default:()=>u,frontMatter:()=>l,metadata:()=>t,toc:()=>a});const t=JSON.parse('{"id":"primitives/Activations/relu","title":"ReLU","description":"Rectified Linear Unit (ReLU)","source":"@site/docs/primitives/Activations/relu.md","sourceDirName":"primitives/Activations","slug":"/primitives/Activations/relu","permalink":"/docs/primitives/Activations/relu","draft":false,"unlisted":false,"editUrl":"https://github.com/neuroscript/neuroscript/tree/main/website/docs/primitives/Activations/relu.md","tags":[],"version":"current","frontMatter":{"sidebar_label":"ReLU"},"sidebar":"docsSidebar","previous":{"title":"Linear","permalink":"/docs/primitives/Basics/linear"},"next":{"title":"GELU","permalink":"/docs/primitives/Activations/gelu"}}');var s=i(4848),r=i(8453);const l={sidebar_label:"ReLU"},o="ReLU",c={},a=[{value:"Signature",id:"signature",level:2},{value:"Ports",id:"ports",level:2},{value:"Implementation",id:"implementation",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",header:"header",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,r.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"relu",children:"ReLU"})}),"\n",(0,s.jsx)(n.p,{children:"Rectified Linear Unit (ReLU)"}),"\n",(0,s.jsx)(n.p,{children:"Simple non-linear activation that outputs max(0, x). One of the most\nwidely used activation functions in deep learning due to its simplicity\nand effectiveness."}),"\n",(0,s.jsx)(n.p,{children:"Shape Contract:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Input: [*shape] arbitrary shape"}),"\n",(0,s.jsx)(n.li,{children:"Output: [*shape] same shape as input"}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"Notes:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Element-wise operation: ReLU(x) = max(0, x)"}),"\n",(0,s.jsx)(n.li,{children:"Non-differentiable at x=0 (subgradient is typically used)"}),"\n",(0,s.jsx)(n.li,{children:"Can suffer from dying ReLU problem (neurons output 0 for all inputs)"}),"\n",(0,s.jsx)(n.li,{children:"Fast to compute, no vanishing gradient for positive inputs"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"signature",children:"Signature"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-neuroscript",children:"neuron ReLU()\n"})}),"\n",(0,s.jsx)(n.h2,{id:"ports",children:"Ports"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Inputs:"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"default"}),": ",(0,s.jsx)(n.code,{children:"[*shape]"})]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Outputs:"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"default"}),": ",(0,s.jsx)(n.code,{children:"[*shape]"})]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"implementation",children:"Implementation"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:'Source { source: "core", path: "activations/ReLU" }\n'})})]})}function u(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}},8453(e,n,i){i.d(n,{R:()=>l,x:()=>o});var t=i(6540);const s={},r=t.createContext(s);function l(e){const n=t.useContext(r);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:l(e.components),t.createElement(r.Provider,{value:n},e.children)}}}]);
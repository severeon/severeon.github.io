"use strict";(globalThis.webpackChunkneuroscript_docs=globalThis.webpackChunkneuroscript_docs||[]).push([[5782],{2829(n,e,i){let o;function t(n,e){return function(n,e){u+=e,u>=c&&(l=new TextDecoder("utf-8",{ignoreBOM:!0,fatal:!0}),l.decode(),u=e);return l.decode(r().subarray(n,n+e))}(n>>>=0,e)}i.d(e,{Ay:()=>x,MQ:()=>_,ns:()=>h,wE:()=>g});let a=null;function r(){return null!==a&&0!==a.byteLength||(a=new Uint8Array(o.memory.buffer)),a}function s(n,e,i){if(void 0===i){const i=m.encode(n),o=e(i.length,1)>>>0;return r().subarray(o,o+i.length).set(i),p=i.length,o}let o=n.length,t=e(o,1)>>>0;const a=r();let s=0;for(;s<o;s++){const e=n.charCodeAt(s);if(e>127)break;a[t+s]=e}if(s!==o){0!==s&&(n=n.slice(s)),t=i(t,o,o=s+3*n.length,1)>>>0;const e=r().subarray(t+s,t+o);s+=m.encodeInto(n,e).written,t=i(t,o,s,1)>>>0}return p=s,t}function d(n){const e=o.__wbindgen_externrefs.get(n);return o.__externref_table_dealloc(n),e}let l=new TextDecoder("utf-8",{ignoreBOM:!0,fatal:!0});l.decode();const c=2146435072;let u=0;const m=new TextEncoder;"encodeInto"in m||(m.encodeInto=function(n,e){const i=m.encode(n);return e.set(i),{read:n.length,written:i.length}});let p=0;function h(n){let e,i;try{const l=s(n,o.__wbindgen_malloc,o.__wbindgen_realloc),c=p,u=o.analyze(l,c);var a=u[0],r=u[1];if(u[3])throw a=0,r=0,d(u[2]);return e=a,i=r,t(a,r)}finally{o.__wbindgen_free(e,i,1)}}function g(n,e){let i,a;try{const m=s(n,o.__wbindgen_malloc,o.__wbindgen_realloc),h=p;var r=null==e?0:s(e,o.__wbindgen_malloc,o.__wbindgen_realloc),l=p;const g=o.compile(m,h,r,l);var c=g[0],u=g[1];if(g[3])throw c=0,u=0,d(g[2]);return i=c,a=u,t(c,u)}finally{o.__wbindgen_free(i,a,1)}}function _(n){let e,i;try{const l=s(n,o.__wbindgen_malloc,o.__wbindgen_realloc),c=p,u=o.list_neurons(l,c);var a=u[0],r=u[1];if(u[3])throw a=0,r=0,d(u[2]);return e=a,i=r,t(a,r)}finally{o.__wbindgen_free(e,i,1)}}const f=new Set(["basic","cors","default"]);function b(){const n={wbg:{}};return n.wbg.__wbindgen_cast_2241b6af4c4b2941=function(n,e){return t(n,e)},n.wbg.__wbindgen_init_externref_table=function(){const n=o.__wbindgen_externrefs,e=n.grow(4);n.set(0,void 0),n.set(e+0,void 0),n.set(e+1,null),n.set(e+2,!0),n.set(e+3,!1)},n}function y(n,e){return o=n.exports,v.__wbindgen_wasm_module=e,a=null,o.__wbindgen_start(),o}async function v(n){if(void 0!==o)return o;void 0!==n&&(Object.getPrototypeOf(n)===Object.prototype?({module_or_path:n}=n):console.warn("using deprecated parameters for the initialization function; pass a single object instead")),void 0===n&&(n=new URL(i(6407),i.b));const e=b();("string"==typeof n||"function"==typeof Request&&n instanceof Request||"function"==typeof URL&&n instanceof URL)&&(n=fetch(n));const{instance:t,module:a}=await async function(n,e){if("function"==typeof Response&&n instanceof Response){if("function"==typeof WebAssembly.instantiateStreaming)try{return await WebAssembly.instantiateStreaming(n,e)}catch(i){if(!n.ok||!f.has(n.type)||"application/wasm"===n.headers.get("Content-Type"))throw i;console.warn("`WebAssembly.instantiateStreaming` failed because your server does not serve Wasm with `application/wasm` MIME type. Falling back to `WebAssembly.instantiate` which is slower. Original error:\n",i)}const o=await n.arrayBuffer();return await WebAssembly.instantiate(o,e)}{const i=await WebAssembly.instantiate(n,e);return i instanceof WebAssembly.Instance?{instance:i,module:n}:i}}(await n,e);return y(t,a)}const x=v},2960(n,e,i){i.r(e),i.d(e,{default:()=>c});var o=i(6540),t=i(3544),a=i(2829);const r=[{id:"mlp",title:"Simple MLP",category:"Basics",description:"Feed-forward network with dimension parameters and expressions",code:"# A simple multi-layer perceptron with expansion and contraction\nneuron MLP(dim):\n  in: [*, dim]\n  out: [*, dim]\n  graph:\n    in ->\n      Linear(dim, dim * 4)\n      GELU()\n      Linear(dim * 4, dim)\n      out",targetNeuron:"MLP",features:["Dimension variables","Pipeline syntax","Dimension expressions"]},{id:"linear-projection",title:"Linear Projection",category:"Basics",description:"Simple projection layer changing dimensions",code:"# Project from one dimension to another\nneuron Projection(in_dim, out_dim):\n  in: [batch, in_dim]\n  out: [batch, out_dim]\n  graph:\n    in -> Linear(in_dim, out_dim) -> out",targetNeuron:"Projection",features:["Shape signatures","Dimension propagation"]},{id:"normalization",title:"Normalization Layer",category:"Basics",description:"Layer normalization with dimension preservation",code:"# Normalize activations along the feature dimension\nneuron NormalizedLayer(dim):\n  in: [*, dim]\n  out: [*, dim]\n  graph:\n    in ->\n      LayerNorm(dim)\n      Linear(dim, dim)\n      out",targetNeuron:"NormalizedLayer",features:["Normalization","Shape preservation"]},{id:"residual",title:"Residual Block",category:"Patterns",description:"Skip connections with Fork and Add primitives",code:"# Classic residual connection pattern\nneuron ResidualBlock(dim):\n  in: [*, dim]\n  out: [*, dim]\n  graph:\n    # Fork creates two copies of the input\n    in -> Fork() -> (main, skip)\n\n    # Process the main path\n    main ->\n      LayerNorm(dim)\n      Linear(dim, dim * 4)\n      GELU()\n      Linear(dim * 4, dim)\n      processed\n\n    # Add skip connection back\n    (processed, skip) -> Add() -> out",targetNeuron:"ResidualBlock",features:["Fork primitive","Tuple unpacking","Residual connections"]},{id:"parallel-paths",title:"Parallel Processing",category:"Patterns",description:"Multiple parallel paths with concatenation",code:"# Process input through three parallel paths\nneuron ParallelPaths(dim):\n  in: [*, dim]\n  out: [*, dim * 3]\n  graph:\n    # Split into three paths\n    in -> Fork3() -> (path1, path2, path3)\n\n    # Process each independently\n    path1 -> Linear(dim, dim) -> p1\n    path2 -> Linear(dim, dim) -> p2\n    path3 -> Linear(dim, dim) -> p3\n\n    # Combine results (concat p1+p2, then concat with p3)\n    (p1, p2) -> Concat(-1) -> temp\n    (temp, p3) -> Concat(-1) -> out",targetNeuron:"ParallelPaths",features:["Multi-way fork","Parallel paths","Concatenation"]},{id:"pre-activation",title:"Pre-Activation Residual",category:"Patterns",description:"Residual with normalization before transformation",code:"# Pre-activation residual block (normalize first)\nneuron PreActResidual(dim):\n  in: [batch, dim]\n  out: [batch, dim]\n  graph:\n    in -> Fork() -> (main, skip)\n\n    main ->\n      LayerNorm(dim)\n      Linear(dim, dim * 4)\n      GELU()\n      Linear(dim * 4, dim)\n      processed\n\n    (processed, skip) -> Add() -> out",targetNeuron:"PreActResidual",features:["Pre-activation","Residual pattern","Named dimensions"]},{id:"match-basic",title:"Shape Pattern Matching",category:"Advanced",description:"Match expressions with dimension capture",code:"# Route based on input dimension size\nneuron AdaptiveProjection:\n  in: [*, dim]\n  out: [*, 512]\n  graph:\n    in -> match:\n      [*, d] where d > 512: Linear(d, 512) -> out\n      [*, d] where d < 512: Linear(d, 512) -> out\n      [*, d]: Identity() -> out",targetNeuron:"AdaptiveProjection",features:["Match expressions","Dimension capture","Guard conditions"]},{id:"match-routing",title:"Shape-Based Routing",category:"Advanced",description:"Different processing paths based on tensor dimensions",code:"# Choose processing based on feature dimension\nneuron DimensionRouter(out_dim):\n  in: [batch, in_dim]\n  out: [batch, out_dim]\n  graph:\n    in -> match:\n      [batch, d] where d > 1024:\n        Linear(d, 512) ->\n        Linear(512, out_dim)\n      [batch, d] where d > 256:\n        Linear(d, out_dim)\n      [batch, d]:\n        Linear(d, out_dim)\n    -> out",targetNeuron:"DimensionRouter",features:["Shape matching","Dimension binding","Multi-line syntax"]},{id:"variadic-shapes",title:"Variadic Wildcards",category:"Advanced",description:"Match variable-length shape prefixes",code:"# Process tensors with arbitrary leading dimensions\nneuron FlexibleNorm(dim):\n  in: [*shape, dim]\n  out: [*shape, dim]\n  graph:\n    in ->\n      LayerNorm(dim)\n      Linear(dim, dim)\n      out",targetNeuron:"FlexibleNorm",features:["Variadic wildcards","Shape prefixes","Rank-agnostic"]},{id:"attention-head",title:"Attention Head",category:"Real World",description:"Single attention head with Q, K, V projections",code:"# Single-head scaled dot-product attention\nneuron AttentionHead(dim, head_dim):\n  in: [batch, seq, dim]\n  out: [batch, seq, head_dim]\n  graph:\n    # Project to Q, K, V\n    in -> Fork3() -> (q_in, k_in, v_in)\n    q_in -> Linear(dim, head_dim) -> q\n    k_in -> Linear(dim, head_dim) -> k\n    v_in -> Linear(dim, head_dim) -> v\n\n    # Compute attention\n    (q, k, v) -> ScaledDotProductAttention(head_dim) -> out",targetNeuron:"AttentionHead",features:["Multi-input operations","Attention mechanism","Fork-join pattern"]},{id:"transformer-ffn",title:"Transformer FFN",category:"Real World",description:"Feed-forward network from transformer",code:"# Transformer feed-forward network with residual\nneuron TransformerFFN(dim):\n  in: [batch, seq, dim]\n  out: [batch, seq, dim]\n  graph:\n    in -> Fork() -> (main, skip)\n\n    main ->\n      LayerNorm(dim)\n      Linear(dim, dim * 4)\n      GELU()\n      Linear(dim * 4, dim)\n      Dropout(0.1)\n      processed\n\n    (processed, skip) -> Add() -> out",targetNeuron:"TransformerFFN",features:["Transformer components","Dropout","Standard architecture"]},{id:"simple-cnn",title:"CNN Block",category:"Real World",description:"Convolutional block with batch norm",code:"# Basic CNN block with conv + norm + activation\nneuron ConvBlock(channels):\n  in: [batch, channels, h, w]\n  out: [batch, channels, h, w]\n  graph:\n    in ->\n      Conv2d(channels, channels, kernel_size=3, padding=1)\n      BatchNorm(channels)\n      ReLU()\n      out",targetNeuron:"ConvBlock",features:["Convolutional layers","Batch normalization","2D operations"]}];var s=i(9350),d=i(4848);function l(){const[n,e]=(0,o.useState)(""),[i,t]=(0,o.useState)(""),[l,c]=(0,o.useState)(""),[u,m]=(0,o.useState)(!1),[p,h]=(0,o.useState)("mlp"),[g,_]=(0,o.useState)([]),[f,b]=(0,o.useState)(null),[y,v]=(0,o.useState)(!1),[x,w]=(0,o.useState)(!1),[k,S]=(0,o.useState)(!1),L=(0,o.useRef)(null),C=function(){const n={};return r.forEach(e=>{n[e.category]||(n[e.category]=[]),n[e.category].push(e)}),n}();(0,o.useEffect)(()=>{(0,a.Ay)().then(()=>{m(!0);const n=r.find(n=>"mlp"===n.id);n&&N(n)}).catch(n=>{console.error("WASM init failed:",n),c("Failed to initialize compiler: "+n)})},[]),(0,o.useEffect)(()=>{const n=()=>{S(window.innerWidth<768)};return n(),window.addEventListener("resize",n),()=>window.removeEventListener("resize",n)},[]);(0,o.useCallback)((n,e)=>{L.current&&clearTimeout(L.current),L.current=setTimeout(()=>{P(n,e)},500)},[]);const P=(0,o.useCallback)((n,e=null)=>{if(u){w(!0);try{const i=n.replace(/^use .*$/gm,"# $&"),o=s.y+"\n"+i,r=(0,a.MQ)(o),d=JSON.parse(r);_(d);const l=e||(1===d.length?d[0]:null),u=(0,a.wE)(o,l);t(u),c(""),!e&&l&&b(l)}catch(i){c(i.toString()),t(""),_([])}finally{w(!1)}}},[u]),N=(0,o.useCallback)(n=>{e(n.code),h(n.id),b(n.targetNeuron),P(n.code,n.targetNeuron)},[P]),j=r.find(n=>n.id===p);return(0,d.jsxs)("div",{style:{display:"flex",flexDirection:"column",gap:"1rem",maxWidth:"100%"},children:[(0,d.jsxs)("div",{style:{display:"flex",gap:"1rem",alignItems:"center",flexWrap:"wrap"},children:[(0,d.jsxs)("div",{style:{flex:"1",minWidth:"250px"},children:[(0,d.jsx)("label",{style:{display:"block",marginBottom:"0.25rem",fontWeight:"bold"},children:"Example:"}),(0,d.jsx)("select",{value:p,onChange:n=>{const e=n.target.value,i=r.find(n=>n.id===e);i&&N(i)},disabled:!u,style:{width:"100%",padding:"0.5rem",borderRadius:"4px",border:"1px solid var(--ifm-color-emphasis-300)",backgroundColor:"var(--ifm-background-color)",color:"var(--ifm-font-color-base)",fontSize:"14px"},children:Object.entries(C).map(([n,e])=>(0,d.jsx)("optgroup",{label:n,children:e.map(n=>(0,d.jsx)("option",{value:n.id,children:n.title},n.id))},n))})]}),g.length>1&&(0,d.jsxs)("div",{style:{minWidth:"200px"},children:[(0,d.jsx)("label",{style:{display:"block",marginBottom:"0.25rem",fontWeight:"bold"},children:"Compile:"}),(0,d.jsxs)("select",{value:f||"",onChange:n=>{const e=n.target.value;b(e)},disabled:!u,style:{width:"100%",padding:"0.5rem",borderRadius:"4px",border:"1px solid var(--ifm-color-emphasis-300)",backgroundColor:"var(--ifm-background-color)",color:"var(--ifm-font-color-base)",fontSize:"14px"},children:[(0,d.jsx)("option",{value:"",children:"Auto-detect"}),g.map(n=>(0,d.jsx)("option",{value:n,children:n},n))]})]}),(0,d.jsx)("div",{style:{display:"flex",alignItems:"flex-end"},children:(0,d.jsx)("button",{onClick:()=>{P(n,f)},disabled:!u||x,className:"button button--primary",style:{fontSize:"14px"},children:x?"\u23f3 Compiling...":"\u25b6\ufe0f Compile"})})]}),j&&(0,d.jsxs)("div",{style:{padding:"0.75rem",backgroundColor:"var(--ifm-color-emphasis-100)",borderRadius:"8px",borderLeft:"4px solid var(--ifm-color-primary)"},children:[(0,d.jsx)("p",{style:{margin:"0 0 0.5rem 0",fontWeight:"bold"},children:j.description}),(0,d.jsx)("div",{style:{display:"flex",gap:"0.5rem",flexWrap:"wrap"},children:j.features.map(n=>(0,d.jsx)("span",{style:{padding:"0.25rem 0.5rem",backgroundColor:"var(--ifm-color-primary)",color:"white",borderRadius:"12px",fontSize:"12px",fontWeight:"500"},children:n},n))})]}),(0,d.jsxs)("div",{style:{display:"flex",gap:"1rem",minHeight:"500px",flexDirection:k?"column":"row"},children:[(0,d.jsxs)("div",{style:{flex:1,display:"flex",flexDirection:"column",minWidth:0},children:[(0,d.jsxs)("div",{style:{display:"flex",justifyContent:"space-between",alignItems:"center",marginBottom:"0.5rem"},children:[(0,d.jsx)("h3",{style:{margin:0},children:"Source (NeuroScript)"}),x&&(0,d.jsx)("span",{style:{fontSize:"12px",color:"var(--ifm-color-primary)"},children:"Compiling..."})]}),(0,d.jsx)("textarea",{value:n,onChange:n=>{const i=n.target.value;e(i),L.current&&(clearTimeout(L.current),L.current=null)},disabled:!u,style:{flex:1,fontFamily:'"Fira Code", "Cascadia Code", "SF Mono", Monaco, "Inconsolata", "Roboto Mono", "Source Code Pro", Menlo, Consolas, "DejaVu Sans Mono", monospace',padding:"1rem",resize:"none",borderRadius:"8px",border:"1px solid var(--ifm-color-emphasis-300)",backgroundColor:"var(--ifm-background-color)",color:"var(--ifm-font-color-base)",fontSize:"13px",lineHeight:"1.6",minHeight:"400px"},spellCheck:"false"})]}),(0,d.jsxs)("div",{style:{flex:1,display:"flex",flexDirection:"column",minWidth:0},children:[(0,d.jsxs)("div",{style:{display:"flex",justifyContent:"space-between",alignItems:"center",marginBottom:"0.5rem"},children:[(0,d.jsx)("h3",{style:{margin:0},children:"Output (PyTorch)"}),i&&(0,d.jsx)("button",{onClick:async()=>{try{await navigator.clipboard.writeText(i),v(!0),setTimeout(()=>v(!1),2e3)}catch(n){console.error("Failed to copy:",n)}},className:"button button--sm button--secondary",style:{fontSize:"12px"},children:y?"\u2713 Copied!":"\ud83d\udccb Copy"})]}),l?(0,d.jsx)("div",{style:{flex:1,backgroundColor:"var(--ifm-color-danger-contrast-background)",color:"var(--ifm-color-danger-contrast-foreground)",padding:"1rem",fontFamily:"monospace",whiteSpace:"pre-wrap",overflow:"auto",borderRadius:"8px",border:"1px solid var(--ifm-color-danger)",fontSize:"13px",lineHeight:"1.6",minHeight:"400px"},children:l}):(0,d.jsx)("div",{style:{flex:1,fontFamily:'"Fira Code", "Cascadia Code", "SF Mono", Monaco, "Inconsolata", "Roboto Mono", "Source Code Pro", Menlo, Consolas, "DejaVu Sans Mono", monospace',padding:"1rem",backgroundColor:"#1e1e1e",color:"#d4d4d4",overflow:"auto",borderRadius:"8px",border:"1px solid #333",whiteSpace:"pre",fontSize:"13px",lineHeight:"1.6",minHeight:"400px"},children:i||"Compiled PyTorch code will appear here..."})]})]}),i&&(0,d.jsxs)("div",{style:{padding:"0.5rem",backgroundColor:"var(--ifm-color-emphasis-100)",borderRadius:"4px",fontSize:"12px",color:"var(--ifm-color-emphasis-700)",textAlign:"right"},children:[g.length," neuron",1!==g.length?"s":""," \u2022 ",i.split("\n").length," lines generated"]})]})}function c(){return(0,d.jsx)(t.A,{title:"Playground",description:"Compile NeuroScript to PyTorch in your browser",children:(0,d.jsx)("main",{children:(0,d.jsxs)("div",{className:"container margin-vert--lg",children:[(0,d.jsx)("h1",{children:"NeuroScript Playground"}),(0,d.jsx)("p",{children:"Write NeuroScript code on the left, see compiled PyTorch code on the right."}),(0,d.jsx)(l,{})]})})})}},6407(n,e,i){n.exports=i.p+"fbb2bcaa77df6bc9.wasm"},9350(n,e,i){i.d(e,{y:()=>o});const o="\n\n\nneuron FFN(dim, expansion):\n    in: [*shape, dim]\n    out: [*shape, dim]\n    graph:\n        in ->\n            Linear(dim, expansion)\n            GELU()\n            Linear(expansion, dim)\n            out\n\nneuron FFNWithHidden(in_dim, hidden_dim, out_dim):\n    in: [*shape, in_dim]\n    out: [*shape, out_dim]\n    graph:\n        in ->\n            Linear(in_dim, hidden_dim)\n            GELU()\n            Linear(hidden_dim, out_dim)\n            out\n\nneuron ParallelFFN(dim):\n    in: [*, dim]\n    out: [*, dim]\n    graph:\n        in -> FFN(dim, dim * 2) -> out\n\nneuron AdaptiveAvgPool(output_size):\n    in: [batch, channels, *, *]\n    out: [batch, channels, output_size, output_size]\n    impl: core,pooling/AdaptiveAvgPool\n\nneuron AdaptiveMaxPool(output_size):\n    in: [batch, channels, *, *]\n    out: [batch, channels, output_size, output_size]\n    impl: core,pooling/AdaptiveMaxPool\n\nneuron Add:\n    in main: [*shape]\n    in skip: [*shape]\n    out: [*shape]\n    impl: core,structural/Add\n\nneuron AvgPool(kernel_size, stride=1, padding=0):\n    in: [batch, channels, height, width]\n    out: [batch, channels, *, *]\n    impl: core,pooling/AvgPool\n\nneuron BatchNorm(num_features):\n    in: [*shape, num_features]\n    out: [*shape, num_features]\n    impl: core,normalization/BatchNorm\n\nneuron Bias(dim):\n    in: [*, dim]\n    out: [*, dim]\n    impl: core,operations/Bias\n\nneuron Concat(dim):\n    in a: [*shape_a]\n    in b: [*shape_b]\n    out: [*shape_out]\n    impl: core,structural/Concat\n\nneuron Conv1d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=true):\n    in: [batch, in_channels, length]\n    out: [batch, out_channels, *]\n    impl: core,convolutions/Conv1d\n\nneuron Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=true):\n    in: [batch, in_channels, height, width]\n    out: [batch, out_channels, *, *]\n    impl: core,convolutions/Conv2d\n\nneuron Conv3d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=true):\n    in: [batch, in_channels, depth, height, width]\n    out: [batch, out_channels, *, *, *]\n    impl: core,convolutions/Conv3d\n\nneuron DepthwiseConv(channels, kernel_size, stride=1, padding=0, dilation=1, bias=true):\n    in: [batch, channels, height, width]\n    out: [batch, channels, *, *]\n    impl: core,convolutions/DepthwiseConv\n\nneuron DropConnect(drop_prob):\n    in: [*shape]\n    out: [*shape]\n    impl: core,regularization/DropConnect\n\nneuron Dropout(p):\n    in: [*shape]\n    out: [*shape]\n    impl: core,regularization/Dropout\n\nneuron DropPath(drop_prob):\n    in: [*shape]\n    out: [*shape]\n    impl: core,regularization/DropPath\n\nneuron Einsum(equation):\n    in a: [*shape_a]\n    in b: [*shape_b]\n    out: [*shape_out]\n    impl: core,operations/Einsum\n\nneuron ELU(alpha=1.0):\n    in: [*shape]\n    out: [*shape]\n    impl: core,activations/ELU\n\nneuron Embedding(num_embeddings, embedding_dim):\n    in: [*, seq_len]\n    out: [*, seq_len, embedding_dim]\n    impl: core,embeddings/Embedding\n\nneuron Flatten(start_dim=1, end_dim=-1):\n    in: [*shape_in]\n    out: [*shape_out]\n    impl: core,structural/Flatten\n\nneuron Fork:\n    in: [*shape]\n    out main: [*shape]\n    out skip: [*shape]\n    impl: core,structural/Fork\n\nneuron Fork3:\n    in: [*shape]\n    out a: [*shape]\n    out b: [*shape]\n    out c: [*shape]\n    impl: core,structural/Fork3\n\nneuron GELU:\n    in: [*shape]\n    out: [*shape]\n    impl: core,activations/GELU\n\nneuron GlobalAvgPool:\n    in: [batch, channels, *, *]\n    out: [batch, channels, 1, 1]\n    impl: core,pooling/GlobalAvgPool\n\nneuron GlobalMaxPool:\n    in: [batch, channels, *, *]\n    out: [batch, channels, 1, 1]\n    impl: core,pooling/GlobalMaxPool\n\nneuron GroupNorm(num_groups, num_channels):\n    in: [*, num_channels, *, *]\n    out: [*, num_channels, *, *]\n    impl: core,normalization/GroupNorm\n\nneuron Identity:\n    in: [*shape]\n    out: [*shape]\n    impl: core,structural/Identity\n\nneuron InstanceNorm(num_features, eps=0.00001, affine=true):\n    in: [batch, num_features, *spatial]\n    out: [batch, num_features, *spatial]\n    impl: core,normalization/InstanceNorm\n\nneuron LayerNorm(dim):\n    in: [*shape, dim]\n    out: [*shape, dim]\n    impl: core,normalization/LayerNorm\n\nneuron LearnedPositionalEmbedding(max_positions, embedding_dim):\n    in: [*, seq_len, embedding_dim]\n    out: [*, seq_len, embedding_dim]\n    impl: core,embeddings/LearnedPositionalEmbedding\n\nneuron Linear(in_dim, out_dim):\n    in: [*, in_dim]\n    out: [*, out_dim]\n    impl: core,nn/Linear\n\nneuron MatMul:\n    in a: [*, n, m]\n    in b: [*, m, p]\n    out: [*, n, p]\n    impl: core,operations/MatMul\n\nneuron MaxPool(kernel_size, stride=1, padding=0, dilation=1):\n    in: [batch, channels, height, width]\n    out: [batch, channels, *, *]\n    impl: core,pooling/MaxPool\n\nneuron Mish:\n    in: [*shape]\n    out: [*shape]\n    impl: core,activations/Mish\n\nneuron MultiHeadSelfAttention(dim, num_heads):\n    in: [*, seq_len, dim]\n    out: [*, seq_len, dim]\n    impl: core,attention/MultiHeadSelfAttention\n\nneuron Multiply:\n    in a: [*shape]\n    in b: [*shape]\n    out: [*shape]\n    impl: core,structural/Multiply\n\nneuron Pad(padding, value=0, mode=constant):\n    in: [*shape_in]\n    out: [*shape_out]\n    impl: core,structural/Pad\n\nneuron PositionalEncoding(dim, max_len):\n    in: [*, seq_len, dim]\n    out: [*, seq_len, dim]\n    impl: core,embeddings/PositionalEncoding\n\nneuron PReLU(num_parameters=1, init=0.25):\n    in: [*shape]\n    out: [*shape]\n    impl: core,activations/PReLU\n\nneuron ReLU:\n    in: [*shape]\n    out: [*shape]\n    impl: core,activations/ReLU\n\nneuron Reshape(target_shape):\n    in: [*shape_in]\n    out: [*shape_out]\n    impl: core,structural/Reshape\n\nneuron RMSNorm(dim):\n    in: [*, dim]\n    out: [*, dim]\n    impl: core,normalization/RMSNorm\n\nneuron RotaryEmbedding(dim, max_position_embeddings=2048, base=10000):\n    in query: [*batch, seq, num_heads, dim]\n    in key: [*batch, seq, num_heads, dim]\n    out q_out: [*batch, seq, num_heads, dim]\n    out k_out: [*batch, seq, num_heads, dim]\n    impl: neuroscript,embeddings/RotaryEmbedding\n\nneuron Scale(dim):\n    in: [*, dim]\n    out: [*, dim]\n    impl: core,operations/Scale\n\nneuron ScaledDotProductAttention(d_k):\n    in query: [*, seq_q, d_k]\n    in key: [*, seq_k, d_k]\n    in value: [*, seq_v, d_v]\n    out: [*, seq_q, d_v]\n    impl: core,attention/ScaledDotProductAttention\n\nneuron SeparableConv(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, bias=true):\n    in: [batch, in_channels, height, width]\n    out: [batch, out_channels, *, *]\n    impl: core,convolutions/SeparableConv\n\nneuron Sigmoid:\n    in: [*shape]\n    out: [*shape]\n    impl: core,activations/Sigmoid\n\nneuron SiLU:\n    in: [*shape]\n    out: [*shape]\n    impl: core,activations/SiLU\n\nneuron Slice(dim, start, end):\n    in: [*shape_in]\n    out: [*shape_out]\n    impl: core,structural/Slice\n\nneuron Softmax(dim):\n    in: [*, dim]\n    out: [*, dim]\n    impl: core,activations/Softmax\n\nneuron Split(num_splits, dim=-1):\n    in: [*shape]\n    out a: [*shape_a]\n    out b: [*shape_b]\n    impl: core,structural/Split\n\nneuron Tanh:\n    in: [*shape]\n    out: [*shape]\n    impl: core,activations/Tanh\n\nneuron Transpose(dims):\n    in: [*shape_in]\n    out: [*shape_out]\n    impl: core,structural/Transpose\n\nneuron TransposedConv(in_channels, out_channels, kernel_size, stride=1, padding=0, output_padding=0, dilation=1, groups=1, bias=true):\n    in: [batch, in_channels, height, width]\n    out: [batch, out_channels, *, *]\n    impl: core,convolutions/TransposedConv\n\nneuron SimpleTransformerBlock(dim):\n    in: [*, dim]\n    out: [*, dim]\n    graph:\n        in ->\n            LayerNorm(dim)\n            Linear(dim, dim)\n            Dropout(0.1)\n            out\n\nneuron TransformerBlock(dim, num_heads, d_ff):\n    in: [batch, seq, dim]\n    out: [batch, seq, dim]\n    graph:\n        in -> Fork() -> (skip1, attn_path)\n        attn_path ->\n            LayerNorm(dim)\n            MultiHeadSelfAttention(dim, num_heads)\n            Dropout(0.1)\n            attn_out\n        (skip1, attn_out) -> Add() -> attn_residual\n        attn_residual -> Fork() -> (skip2, ffn_path)\n        ffn_path ->\n            LayerNorm(dim)\n            FFN(dim, d_ff)\n            Dropout(0.1)\n            ffn_out\n        (skip2, ffn_out) -> Add() -> out\n\nneuron TransformerStack2(d_model, num_heads, d_ff):\n    in: [*, d_model]\n    out: [*, d_model]\n    graph:\n        in ->\n            SimpleTransformerBlock(d_model)\n            SimpleTransformerBlock(d_model)\n            out\n\nneuron SequentialTransformer(d_model, num_heads, d_ff):\n    in: [*, d_model]\n    out: [*, d_model]\n    graph:\n        in ->\n            SimpleTransformerBlock(d_model)\n            out\n"}}]);